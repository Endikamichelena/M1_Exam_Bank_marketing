# -*- coding: utf-8 -*-
"""M1-P1-Bank_marketing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x2oDTOv1RWXv51X-Yat3FesdNiqyWL-q

# 0. Data cleaning
"""

## libraries
import pandas as pd
import numpy as np

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# dummies
from sklearn.preprocessing import OneHotEncoder
import itertools

# preprocessing
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

# dimensionality reduction
from sklearn.decomposition import PCA

# clusters
from sklearn.cluster import KMeans

# model
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

df = pd.read_csv('bank_marketing.csv')

# split the names of the columns and the rows
names = df.columns.str.split(';').tolist()
df = df.iloc[:, 0].str.split(';', expand=True)
df.columns = names

# from multi-index to simple index
df.columns = df.columns.get_level_values(0)

# replacing the " " from the columns names.
df.columns = df.columns.str.replace('(["])', '')

# removing " " from the rows
df['job'] = df.job.str.replace('"', '')
df['marital'] = df.marital.str.replace('"', '')
df['education'] = df.education.str.replace('"', '')
df['default'] = df.default.str.replace('"', '')
df['housing'] = df.housing.str.replace('"', '')
df['loan'] = df.loan.str.replace('"', '')
df['contact'] = df.contact.str.replace('"', '')
df['month'] = df.month.str.replace('"', '')
df['day_of_week'] = df.day_of_week.str.replace('"', '')
df['duration'] = df.duration.str.replace('"', '')
df['campaign'] = df.campaign.str.replace('"', '')
df['pdays'] = df.pdays.str.replace('"', '')
df['previous'] = df.previous.str.replace('"', '')
df['poutcome'] = df.poutcome.str.replace('"', '')
df['y'] = df.y.str.replace('"', '')

# changing data type
df["age"] = df["age"].astype(int)
df["duration"] = df["duration"].astype(int)
df["campaign"] = df["campaign"].astype(int)
df["pdays"] = df["pdays"].astype(int)
df["previous"] = df["previous"].astype(int)
df["emp.var.rate"] = df["emp.var.rate"].astype(float)
df["cons.price.idx"] = df["cons.price.idx"].astype(float)
df["cons.conf.idx"] = df["cons.conf.idx"].astype(float)
df["euribor3m"] = df["euribor3m"].astype(float)
df["nr.employed"] = df["nr.employed"].astype(float)

# renaming the similar indexes to avoid feature problems 
df.loc[ df['education'] == 'unknown', 'education'] = 'no info'
df.loc[ df['job'] == 'unknown', 'job'] = 'no answer'
df.loc[ df['housing'] == 'unknown', 'housing'] = 'no registers'

# convert the y column to int. 0 means no and 1 means yes.
df['y'] = df['y'].map({'yes': 1, 'no': 0})
# rename the column.
df.rename(columns = {'y':'result'}, inplace = True)

df

"""# 1. EDA"""

# plot to check the density of the age variable in our df. People is around 30-40 years old.
df['age'].plot.density()

# very similar plot, but we add the variable of the result of the campaign.
sns.displot(data=df, 
            x="age",
            kind="kde",
            hue="result",
            multiple="stack")

# we can see that as the euribor is low the number of campaign intent is to invest. Also, when it is higher it is no, no investment. we don't have much data at euribor around 3.
sns.displot(data=df, 
            x="euribor3m",
            kind="kde",
            hue="result")

# let's see the correlation between variables:
# emp.var.rate & cons.price.idx 
# emp.var.rate & euribor3m
# emp.var.rate & nr.employed
# cons.price.idx & euribor3m
# cons.price.idx & nr. employed
# euribor3m & nr.employed

sns.heatmap(df.corr());

df_success_camp = df[df['result'] == 1]

# a graph to show up the relation between variables.
# pares de grafos para visualizar relaciones entre variables
# test = df_success_camp[['euribor3m','age']]
# sns.pairplot(test)

df_success_camp.head()

# catplot to check the different job position between the people who invested.
g = sns.catplot(data=df_success_camp, 
            x="job", 
            kind="count")
g.set_xticklabels(rotation=90)

g = sns.catplot(data=df_success_camp, 
            x="education", 
            kind="count")
g.set_xticklabels(rotation=90)

sns.boxplot(x=df_success_camp["age"])

# catplot to check the marital status of the people who did invest.
g = sns.catplot(data=df_success_camp, 
            x="marital", 
            kind="count")
g.set_xticklabels(rotation=90)

df_toplot = df[['education', 'job', 'month', 'age', 'result']]
df_toplot_1 = df_toplot[df_toplot.result == 1]
df_toplot_1.head()

df_education = pd.DataFrame(df_toplot_1.groupby('education')['job'].value_counts())
df_education.rename(columns = {'job':'count'}, inplace = True)
df_education.reset_index()

df_education_counts = pd.DataFrame(df_toplot_1['education'].value_counts())
df_education_counts

"""# 2. Unsupervised ML

## Creating dummies
"""

# we select the data that we think is more valuable for our model. 
selected_df_uml = df[['education', 'job', 'age', 'duration', 'result']]

selected_df_uml.head()

# choose the data we need at this point. As we are doing UML, we don't have to work with the result, that's not the purpose.
X = selected_df_uml.iloc[:,:4]

# y = selected_df_uml.result

# we use OneHotEncoder to create binary columns for categorical values, such as marital, education...
ohe_X = OneHotEncoder(sparse=False)

# ohe has to be implemented only in the categorical values.
X_ohe = ohe_X.fit_transform(X.iloc[:,:2])

X_ohe

# 27 columns belong to the sum of the different values of the columns education, job, housing, loan...
X_ohe.shape

columns_X_ohe = list(itertools.chain(*ohe_X.categories_))

X_cat = pd.DataFrame(X_ohe, columns = columns_X_ohe )

X_cat

"""## Preprocessing data

We can only preprocess the numerical data from our selected data frame. In this case "euribor3m" and "age". StandardScaler removes the mean and scales each feature/variable to unit variance, it is necessary for clusterization.
"""

scaler = StandardScaler()

selected_df_uml.age

# selecting nummerical values only.
transformed_nummerical = scaler.fit_transform(X.iloc[:, 2:4])

X.iloc[:, 2:4] = transformed_nummerical

# Standard Scaler is now applied to the nummerical variables.
X

X.index = range(len(X))
X_cat.index = range(len(X_cat))

# joining the dummies and the Standar Scaled variables, after we can start with the PCA trials.
X_enc = X.iloc[:,2:4].join(X_cat)
X_enc

# we also create Y_enc which takes the dummies and non Standar Scaled variables.
Y_enc = selected_df_uml.iloc[:,2:4].join(X_cat)
Y_enc

# pearson coefficient, not well working with this data set, but very usefull with more nummerical data frames.
# btw we used Y_enc.
# Y_enc.corr(method='pearson')

df_to_cluster_scaled = X_enc

"""## PCA

We have to use X_enc or df_to_cluster_scaled in order to apply the dimensionality reduction with PCA.
"""

# we can do some trials with the n_components and check below wich one is the best.
pca = PCA(n_components=7)

#fit-transform the data
df_reduced_pca = pca.fit_transform(df_to_cluster_scaled)

# here is the explanation of how much importance has each variable, better summarized below.
# we can distinguish the seven variables, inthe them 
print(pca.components_)

# this explains the same as before but summarized. if we sum each percentage we see that the 7 components get the 73% of the data. 27% is missing doing this dim. reduction.
# 7 is a high number of variables, so it doesn't make sense to add more variables. we can deduce that PCA isn't the best way to reduce dimensions in this data set.
print(pca.explained_variance_ratio_)

# we can see that the first component understands correctly the age, but the second one is not able to interpret the euribor3m. 
# the third variable understands the no and does not understand the yes. It occurs because they are interpreted as opposite.
# as we see here, PCA is not very useful because the heat map is almost full pink, stucked at 0.
plt.figure(figsize=(18,2))
sns.heatmap(pd.DataFrame(pca.components_, columns=df_to_cluster_scaled.columns), annot=True)

# we can now plot the reduced data
sns.scatterplot(df_reduced_pca[:,0],df_reduced_pca[:,1])

"""## Clustering"""

# if we check the Elbow method we can know which n_clusters is the best.
clusterer = KMeans(n_clusters=3)

Sum_of_squared_distances = []
K = range(1,10)
for k in K:
    km = KMeans(n_clusters=k)
    km = km.fit(df_reduced_pca)
    Sum_of_squared_distances.append(km.inertia_)

plt.plot(K, Sum_of_squared_distances, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

clusterer.fit(df_reduced_pca)

# we add the cluster col to the data frame, but remember, the one with out the Standard Scaled values!
Y_enc['cluster'] = clusterer.labels_
df_Y = selected_df
df_Y['cluster'] = clusterer.labels_

# the cluster column is in the end.
Y_enc.head()

Y_enc.groupby(['cluster']).describe().transpose()

# this is the selected data frame with the clusterization
df_Y.head()

# the 0 cluster is a bit older than the people from the first one.
sns.boxplot(x='cluster', y='age', data=df_Y)

# plt.figure(figsize=(10,10))
# sns.countplot(x = "cluster", hue = "job", data= df_Y)

plt.figure(figsize=(10,10))
df1 = (df_Y
.groupby('cluster')['job']
.value_counts(normalize=True)
.mul(100)
.rename('percent')
.reset_index())
sns.catplot(x='cluster', y='percent', hue='job',kind = 'bar', data=df1)

plt.figure(figsize=(10,10))
df1 = (df_Y
.groupby('cluster')['education']
.value_counts(normalize=True)
.mul(100)
.rename('percent')
.reset_index())
sns.catplot(x='cluster', y='percent', hue='education',kind = 'bar', data=df1)

# plt.figure(figsize=(10,10))
# sns.countplot(x = "cluster", hue = "housing", data= df_Y)
# no se pueden tirar conclusiones entre clusters xq hay diferentes cantidad de datos.

"""# Supervised ML

## Feature engineering

### Creating dummies
"""

selected_df_sml = df[['marital', 'education', 'job', 'poutcome', 'previous', 'pdays', 'campaign', 'age', 'result']]

selected_df_sml.head()

X1 = selected_df_sml.iloc[:,:8]

ohe_X1 = OneHotEncoder(sparse=False)

X1_ohe = ohe_X1.fit_transform(X1.iloc[:,:4])

X1_ohe

X1_ohe.shape

columns_X1_ohe = list(itertools.chain(*ohe_X1.categories_))

X1_cat = pd.DataFrame(X1_ohe, columns = columns_X1_ohe )

X1_cat.head()

"""### Preprocessing data"""

scaler = StandardScaler()

transformed_nummerical = scaler.fit_transform(X1.iloc[:, 4:8])

X1.iloc[:, 4:8] = transformed_nummerical

X1.head()

X1.index = range(len(X1))
X1_cat.index = range(len(X1_cat))

X1_enc = X1.iloc[:,4:8].join(X1_cat)
X1_enc.head()

Y1_enc = selected_df_sml.iloc[:,4:8].join(X1_cat)
Y1_enc.head()

"""### Random undersampling"""

from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

X2 = X1_enc.iloc[:,:]
y2 = selected_df_sml.iloc[:,-1]

X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2,test_size=0.30)

print("Before undersampling: ", Counter(y2_train))

undersample = RandomUnderSampler(sampling_strategy='majority')

X2_train_under, y2_train_under = undersample.fit_resample(X2_train, y2_train)

print("After undersampling: ", Counter(y2_train_under))

from sklearn.svm import SVC
from sklearn.metrics import classification_report, roc_auc_score

model=SVC()
clf_under = model.fit(X2_train_under, y2_train_under)
pred_under = clf_under.predict(X2_test)

print("ROC AUC score for undersampled data: ", roc_auc_score(y2_test, pred_under))

"""## Fitting SML model"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(multi_class="ovr")
model.fit(X2_train, y2_train)

model.score(X2_train, y2_train)

"""## Pick model and train it + itterate"""

import xgboost
from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X2_train, y2_train)

# final evaluation
model.score(X2_test, y2_test)

"""## Evaluation"""

y_pred = model_xgb.predict(X2_test)

mean_squared_error(y2_test, y_pred, squared=False)

feat_importances = pd.Series(model_xgb.feature_importances_, index=X1_enc.columns)
feat_importances.nlargest(20).plot(kind='barh')

!pip install shap

import shap

explainer = shap.TreeExplainer(model_xgb)

shap_values = explainer.shap_values(X1_enc)

shap.summary_plot(shap_values, X1_enc, plot_type="bar")

shap.summary_plot(shap_values, X1_enc)

shap.dependence_plot("age", shap_values, X1_enc)

shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[1,:], X1_enc.iloc[1,:])

"""## ELI5"""

!pip install eli5
import eli5
eli5.show_weights(model, feature_names = X2.columns.tolist())

from eli5 import show_prediction
show_prediction(model, X2_train.iloc[1], feature_names = X2.columns.tolist(), 
                show_feature_values=True)